INFO: To update the model periodically with new data, simply set up a cron job to call `pio train` and `pio deploy`. The engine will continue to serve prediction results during the re-train process. After the training is completed, `pio deploy` will automatically shutdown the existing engine server and bring up a new process on the same port.

INFO: **Note that if you import *large* data set** and the training seems taking forever or getting stuck, it's likely that there is not enough executor memory. It's recommended to setup Spark standalone cluster, specify more driver and executor memory when train with large data set. Please see [FAQ here](/resources/faq/#engine-training) for instructions.

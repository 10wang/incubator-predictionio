# Tutorial 2 - Test Engine Components

During development, you may want to run each component step by step to test out the data pipeline. In this tutorial, we will demonstrate how to do it easily.

## Test Run DataSource

In `engines/src/main/java/recommendations/tutorial2`, you can find `Runner1.java`. It is a small program that uses `JavaSimpleEngineBuilder` to build an engine and uses `JavaAPIDebugWorkflow` to run the workflow.

To test the *DataSource* component, we can simply create an Engine with the *DataSource* component only and leave other components empty:

```java
private static class HalfBakedEngineFactory implements IEngineFactory {
  public JavaSimpleEngine<TrainingData, Object, Query, Float, Object> apply() {
    return new JavaSimpleEngineBuilder<
      TrainingData, Object, Query, Float, Object> ()
      .dataSourceClass(DataSource.class)
      .build();
  }
}
```
Similarily, we only need to add the `DataSourceParams` to `JavaEngineParamsBuilder`.

```java
JavaEngineParams engineParams = new JavaEngineParamsBuilder()
  .dataSourceParams(new DataSourceParams(filePath))
  .build();
```

Then, you can run this Engine by using `JavaAPIDebugWorkflow`.

```java
JavaAPIDebugWorkflow.runEngine(
  "MyEngine",
  new HashMap<String, String>(),
  3, // verbose
  (new HalfBakedEngineFactory()).apply(),
  engineParams,
  null,
  new EmptyParams()
);
```

For quick testing purpose, a very simple test data is provided in data/test/ratings.csv. Each row of the file represents user ID, item ID, and the rating value:

```
1,1,2
1,2,3
1,3,4 ...
```

The `Runner1.java` takes the path of the rating file as argument. Execute the following command to run (The `bin/pio-run` command will automatically compile and package the jars):

```
bin/pio-run io.prediction.engines.java.recommendations.tutorial2.Runner1 data/test/ratings.csv
```

If it runs successfully, you should see the following console output at the end. It prints out the `TrainigData` generated by DataSource.

```
14/07/24 14:43:12 INFO SparkContext: Job finished: collect at DebugWorkflow.scala:409, took 0.022313 s
14/07/24 14:43:12 INFO APIDebugWorkflow$: Data Set 0
14/07/24 14:43:12 INFO APIDebugWorkflow$: Params: Empty
14/07/24 14:43:12 INFO APIDebugWorkflow$: TrainingData:
14/07/24 14:43:12 INFO APIDebugWorkflow$: [TrainingData: [(1,1,2.0), (1,2,3.0), (1,3,4.0), (2,3,4.0), (2,4,1.0), (3,2,2.0), (3,3,1.0), (3,4,3.0), (4,1,5.0), (4,2,3.0), (4,4,2.0)]]
14/07/24 14:43:12 INFO APIDebugWorkflow$: TestingData: (count=0)
14/07/24 14:43:12 INFO APIDebugWorkflow$: Data source complete
14/07/24 14:43:12 INFO APIDebugWorkflow$: Preparator is null. Stop here
```

As you can see, it stops after running the *DataSource* component and it prints out *Training Data* for debugging.

## Test Run Algorithm

By simply adding `addAlgorithmClass()` and `addAlgorithmParams()` in the `JavaSimpleEngineBuilder` and `JavaEngineParamsBuilder`, you can test the `Algorithm` class in the workflow as well, as shown in `Runner2.java`:

```java
private static class HalfBakedEngineFactory implements IEngineFactory {
  public JavaSimpleEngine<TrainingData, Object, Query, Float, Object> apply() {
    return new JavaSimpleEngineBuilder<
      TrainingData, Object, Query, Float, Object> ()
      .dataSourceClass(DataSource.class)
      .preparatorClass() // Use default Preparator
      .addAlgorithmClass("MyRecommendationAlgo", Algorithm.class) // Add Algorithm
      .build();
  }
}
```

```java
JavaEngineParams engineParams = new JavaEngineParamsBuilder()
  .dataSourceParams(new DataSourceParams(filePath))
  .addAlgorithmParams("MyRecommendationAlgo", new AlgoParams(0.2)) // Add Algorithm Params
  .build();
```

Execute the following command to run:

```
bin/pio-run io.prediction.engines.java.recommendations.tutorial2.Runner2 data/test/ratings.csv
```

You should see the *Model* generated by the Algorithm at the end of the console output:

```
14/07/24 15:38:08 INFO TaskSetManager: Finished TID 15 in 31 ms on localhost (progress: 4/4)
14/07/24 15:38:08 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
14/07/24 15:38:08 INFO DAGScheduler: Stage 3 (collect at DebugWorkflow.scala:66) finished in 0.035 s
14/07/24 15:38:08 INFO SparkContext: Job finished: collect at DebugWorkflow.scala:66, took 0.047056 s
14/07/24 15:38:08 INFO APIDebugWorkflow$: [Model: [itemSimilarity: {1={0; 0.8313979616; 0.2586032735; 0.496291667}, 2={0.8313979616; 0; 0.5195887333; 0.6837634588}, 3={0.2586032735; 0.5195887333; 0; 0.3256694736}, 4={0.496291667; 0.6837634588; 0.3256694736; 0}}]
[userHistory: {1={2; 3; 4; 0}, 2={0; 0; 4; 1}, 3={0; 2; 1; 3}, 4={5; 3; 0; 2}}]]
14/07/24 15:38:08 INFO APIDebugWorkflow$: Serving is null. Stop here
```

By adding each component step by step, we can easily test and debug the data pipeline.

Next: [Tutorial 3 - Evaluation](tutorial3-evaluation.md)
